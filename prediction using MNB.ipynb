{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2c6534-9dbb-48e3-ab83-02c8dd3ffd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c9f001-0ac4-4c2e-aa37-c3434a1f91c8",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef1caf-b796-4fb9-96d8-dd00f9fe5a17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('training.csv', encoding = 'ISO-8859-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec39c99-9234-43f7-a6b7-ec4059a815d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['target','ID','date','flag','user','text']\n",
    "df = pd.read_csv('training.csv', names = column_names, encoding = 'ISO-8859-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2069c579-02b7-40db-b8ca-4b772092f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7865b4e3-e29f-4f07-a296-7f45793cc8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].value_counts()\n",
    "# first 8 million tweets are negative and next 8 million tweets are positive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18446a0-f4b9-46dd-a74d-9c67a563cbb2",
   "metadata": {},
   "source": [
    "##### converting the value of 4 to 1 in the target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbb3024-9bf7-4ffc-9c0a-2d5608d31793",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace({'target':{4:1}}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e272ec-76e3-4294-b39c-ddaf3412b3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd54a26-92db-4cf5-a165-9b8491c01f71",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bc5f2a-90e1-4a90-bf33-02a55e6f7325",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to text to lower case\n",
    "df['text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb76338-80c2-4538-b0b6-e84a07e670d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing all urls from text\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'http\\S+|www\\S+|https\\S+', '', x, flags=re.MULTILINE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c4e391-fc8f-4bd0-949b-67e6bcab3bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing symbols and punctuations\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac9341b-9619-4ce8-8b8e-7f416655316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing numbers as it is not relevant here\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\d+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6f3950ca-f7b5-4979-b934-1396d12309bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ID</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>predicted_sentiment</th>\n",
       "      <th>true_sentiment_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>switchfoot   a thats a bummer  you shoulda got...</td>\n",
       "      <td>switchfoot thats bummer shoulda get david carr...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "      <td>upset cant update facebook texting might cry r...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>kenichan i dived many times for the ball manag...</td>\n",
       "      <td>kenichan dive many time ball manage save rest ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>nationwideclass no its not behaving at all im ...</td>\n",
       "      <td>nationwideclass behave im mad cant see</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          ID                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \\\n",
       "0  _TheSpecialOne_  switchfoot   a thats a bummer  you shoulda got...   \n",
       "1    scotthamilton  is upset that he cant update his facebook by t...   \n",
       "2         mattycus  kenichan i dived many times for the ball manag...   \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire    \n",
       "4           Karoli  nationwideclass no its not behaving at all im ...   \n",
       "\n",
       "                                              tokens predicted_sentiment  \\\n",
       "0  switchfoot thats bummer shoulda get david carr...            negative   \n",
       "1  upset cant update facebook texting might cry r...            negative   \n",
       "2  kenichan dive many time ball manage save rest ...            positive   \n",
       "3                    whole body feel itchy like fire            negative   \n",
       "4             nationwideclass behave im mad cant see            negative   \n",
       "\n",
       "  true_sentiment_mapped  \n",
       "0              negative  \n",
       "1              negative  \n",
       "2              negative  \n",
       "3              negative  \n",
       "4              negative  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d7fd47-4f74-4330-bbd0-b7596da58908",
   "metadata": {},
   "source": [
    "### Tokenization & Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1dc67b-2b75-417c-a7bd-002282efd2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'text' \n",
    "df['tokens'] = df[column].apply(word_tokenize)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a47aeb5-d1c9-4c74-b991-eee544a28d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for index, tokens in df['tokens'].items():\n",
    "    df.at[index, 'tokens'] = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ee5d50-d409-45b2-b6e0-657e49d8c1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54f1d8a-3e72-4198-b8bf-46ac79911442",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to map NLTK POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun if no match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d6058f-2001-4555-97b5-88b968b4b9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tokens(tokens):\n",
    "    pos_tags = nltk.pos_tag(tokens)  # Get POS tags for tokens\n",
    "    lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "    return lemmatized\n",
    "\n",
    "# Apply lemmatization to the 'tokens' column\n",
    "df['tokens'] = df['tokens'].apply(lemmatize_tokens)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a4a50f-7676-4e02-9a6e-2ac9e174a962",
   "metadata": {},
   "source": [
    "##### Since the tokens column is a list of tokens and tf-idf (Term Frequency-Inverse Document Frequency) requires string input, we will convert it into string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f9587c-e671-428d-b936-7342056be731",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['tokens'].apply(lambda tokens: ' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aff8c10-6c19-424f-a95a-054d1aa5b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['tokens'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d03c1c7-478f-4d70-95c1-f779137403f8",
   "metadata": {},
   "source": [
    "### Train_test_split and vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b832c4d-b3d6-4a6f-a28e-9a171d466682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['tokens'].values\n",
    "Y = df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adccb18c-79df-4580-9372-d5a92841f892",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ee29dd-db4d-4e30-9ca8-3d1986454b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bb9932-f18a-48d0-8f99-851eb584bf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=1/3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ead3b8-b7ad-4f24-b279-187f468e5e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "X_val = tfidf_vectorizer.transform(X_val)\n",
    "X_test = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473fedcd-7da3-4dfb-a11b-a5c45bf5d81b",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b498093-5316-422f-9be0-6f503a6d65d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f0d22fdd-43f6-4692-9010-bf14a62fc31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Performance:\n",
      "Accuracy: 0.75758125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.80      0.77    159532\n",
      "           1       0.78      0.72      0.75    160468\n",
      "\n",
      "    accuracy                           0.76    320000\n",
      "   macro avg       0.76      0.76      0.76    320000\n",
      "weighted avg       0.76      0.76      0.76    320000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, Y_train)\n",
    "\n",
    "# Prediction on validation set\n",
    "y_val_pred = nb_model.predict(X_val)\n",
    "\n",
    "# evaluation of model on validation set\n",
    "print(\"Validation Performance:\")\n",
    "accuracy = accuracy_score(Y_val, y_val_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(classification_report(Y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ce86dea1-d1b8-4b48-8b6d-9b008889c55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Performance:\n",
      "Accuracy: 0.75900625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.80      0.77     79829\n",
      "           1       0.78      0.72      0.75     80171\n",
      "\n",
      "    accuracy                           0.76    160000\n",
      "   macro avg       0.76      0.76      0.76    160000\n",
      "weighted avg       0.76      0.76      0.76    160000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "y_test_pred = nb_model.predict(X_test)\n",
    "\n",
    "# Evaluathetion of model on the test set\n",
    "print(\"Test Performance:\")\n",
    "accuracy = accuracy_score(Y_test, y_test_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(classification_report(Y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c259b9-797a-4657-8e41-ba9c613462aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'alpha': [0.1, 0.5, 1, 5, 10]}\n",
    "\n",
    "# Grid Search for optimal alpha\n",
    "grid_search = GridSearchCV(MultinomialNB(), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(f\"Best Alpha: {grid_search.best_params_}\")\n",
    "\n",
    "# Evaluate the best model\n",
    "best_nb_model = grid_search.best_estimator_\n",
    "y_val_pred = best_nb_model.predict(X_val)\n",
    "print(\"Validation Performance (After Tuning):\")\n",
    "print(classification_report(Y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4504f881-6b97-40e8-b376-577b97a2de2a",
   "metadata": {},
   "source": [
    "## Testing on user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ea239ab9-7034-4d1b-8210-4d7f71d8809c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    lemmatized_tokens = lemmatize_tokens(tokens)\n",
    "    \n",
    "    # Join tokens back into a single string (optional, based on vectorizer's requirements)\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "sentiment_map = {0: 'negative', 1: 'positive'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "84a691d6-0309-48f5-8924-8b5f37d00050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a sentence for sentiment analysis (or type 'exit' to quit):  my good luck is very bad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: positive\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a sentence for sentiment analysis (or type 'exit' to quit):  my bad luck is very good\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: positive\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a sentence for sentiment analysis (or type 'exit' to quit):  my bad luck is always bad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: negative\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a sentence for sentiment analysis (or type 'exit' to quit):  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting the program.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Take user input\n",
    "    user_input = input(\"Enter a sentence for sentiment analysis (or type 'exit' to quit): \")\n",
    "\n",
    "    if user_input.lower() == 'exit':\n",
    "        print(\"Exiting the program.\")\n",
    "        break\n",
    "\n",
    "    # Preprocess the input\n",
    "    preprocessed_sentence = preprocess_text(user_input)\n",
    "\n",
    "    # Vectorize the input using the trained TfidfVectorizer\n",
    "    X_input = tfidf_vectorizer.transform([preprocessed_sentence])\n",
    "\n",
    "    # Predict sentiment\n",
    "    predicted_sentiment = nb_model.predict(X_input)[0]  # Get the first (and only) prediction\n",
    "\n",
    "    # Map the prediction to its corresponding sentiment label\n",
    "    sentiment_label = sentiment_map[predicted_sentiment]\n",
    "\n",
    "    print(f\"Predicted Sentiment: {sentiment_label}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2602cc-b483-4da0-845f-9e8d510a4a51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
